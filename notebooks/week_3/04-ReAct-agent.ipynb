{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891e21da",
   "metadata": {},
   "source": [
    "### ReAct Agent by providing RAG pipeline as tool to retrieve Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PayloadSchemaType, PointStruct, SparseVectorParams, Document, Prefetch, FusionQuery\n",
    "from qdrant_client.http.models import models\n",
    "import pandas as pd\n",
    "import openai\n",
    "import fastembed\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.messages import convert_to_openai_messages, convert_to_messages\n",
    "\n",
    "from jinja2 import Template\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from instructor import from_openai\n",
    "from openai import OpenAI\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from utils.utils import get_tool_descriptions, format_ai_message\n",
    "from typing import Annotated, List, Any, Dict\n",
    "from pydantic import Field, BaseModel\n",
    "from operator import add\n",
    "import instructor\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943455b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Any, Dict\n",
    "from pydantic import Field\n",
    "from operator import add\n",
    "\n",
    "class Toolcall(BaseModel):\n",
    "    name: str\n",
    "    args: dict\n",
    "    \n",
    "class SearchAgentResponse(BaseModel):\n",
    "    answer: str\n",
    "    tool_calls: List[Toolcall] = Field(default_factory=list)\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add_messages] = []\n",
    "    user_query: str\n",
    "    expanded_queries: List[str] = []\n",
    "    answer: str = \"\"\n",
    "    retrieved_contextdata: Annotated[List[Any], add] = []\n",
    "    query_relevant: bool = False\n",
    "\n",
    "class QueryRelevanceResponse(BaseModel):\n",
    "    query_relevant: bool\n",
    "    reason: str\n",
    "\n",
    "class QueryRewriteResponse(BaseModel):\n",
    "    search_queries: List[str]\n",
    "    \n",
    "class AggregationResponse(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the question in a list format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49434bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "   \n",
    "    response = openai.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "        \n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a75165",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"query_rewriter_node\", \n",
    "description=\"This function rewrites the query to be more specific to include multiple statements\",\n",
    "run_type=\"prompt\"\n",
    ")\n",
    "def query_rewriter_node(state: State) -> str:\n",
    "    \"\"\"\n",
    "    This function rewrites the query to be more specific to include multiple statements\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a Shopping Intent Extraction Agent.\n",
    "    Your task is to analyze a user's complex request and split it into distinct, standalone product search statements.\n",
    "\n",
    "    ### INSTRUCTIONS\n",
    "    1. **Identify Entities:** Look for distinct people or needs mentioned (e.g., \"for me\", \"for my kid\", \"for my wife\").\n",
    "    2. **Segment Requests:** If the user asks for multiple different items, separate them completely. Do not combine them.\n",
    "    3. **Refine & Standardize:** Convert colloquial phrases into clear, searchable product categories.\n",
    "        - If the user says \"nice toys\", convert to \"toys for kids\" or specific categories if implied.\n",
    "        - If the user says \"I need\", infer the target audience based on the context (e.g., \"for adults\" or \"men/women\").\n",
    "    4. **Output List:** Return a raw JSON object containing the list of search statements.\n",
    "    5. Note: Don't includ below examples in your output.\n",
    "\n",
    "    ### EXAMPLES\n",
    "\n",
    "    <Question>\n",
    "    I need smart watch. My kid needs nice toys. My wife want home appliances.\n",
    "    </Question>\n",
    "    <Response>\n",
    "    {\n",
    "        \"search_queries\": [\n",
    "            \"Smart watch for adults\",\n",
    "            \"Toys for kids\",\n",
    "            \"Home appliances for women\"\n",
    "        ]\n",
    "    }\n",
    "    </Response>\n",
    "\n",
    "    <Question>\n",
    "    Looking for a gaming laptop for myself and a pink ipad for my daughter.\n",
    "    </Question>\n",
    "    <Response>\n",
    "    {\n",
    "        \"search_queries\": [\n",
    "            \"Gaming laptop\",\n",
    "            \"Pink iPad for girls\"\n",
    "        ]\n",
    "    }\n",
    "    </Response>\n",
    "\n",
    "    <Question>\n",
    "    {{ query }}\n",
    "    </Question>\n",
    "    <Response>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = Template(prompt_template).render(query=state.user_query)\n",
    "    \n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    \n",
    "    response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=QueryRewriteResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.4\n",
    "    )\n",
    "    return {\n",
    "        \"expanded_queries\": response.search_queries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5352500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making retrieve_embedding_data a retriever node for parallel execution\n",
    "from typing import Dict\n",
    "\n",
    "@tool(\"retrieve_embedding\", description=\"Retrieve embedding data from Qdrant for a given query\", response_format=\"content_and_artifact\")\n",
    "def retrieve_embedding(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of relevant product context strings from a Qdrant database using hybrid search (embedding and BM25 fusion) based on the given user query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's search query for desired product(s).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Each string contains the product ID, description, and average rating, formatted as:\n",
    "            'Product ID: <ASIN> - Description: <description> - Rating: <rating>'\n",
    "    \"\"\"\n",
    "    \n",
    "    qd_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "    \n",
    "    collection_name = \"amazon_items-collection-hybrid-02\"\n",
    "    k=5\n",
    "    \n",
    "    querry_embeddings = create_embeddings(query)\n",
    "    \n",
    "    response = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[Prefetch(\n",
    "            query=querry_embeddings,\n",
    "            using=\"text-embedding-3-small\",\n",
    "            limit=20),\n",
    "            Prefetch(\n",
    "                query=Document(text=query, model=\"qdrant/bm25\"),\n",
    "                using=\"bm25\",\n",
    "                limit=20)\n",
    "            ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=k,\n",
    "    )\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    retrieved_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "    \n",
    "    for point in response.points:\n",
    "        retrieved_context_ids.append(point.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(point.payload[\"description\"])\n",
    "        retrieved_scores.append(point.score)\n",
    "        retrieved_context_ratings.append(point.payload[\"average_rating\"])\n",
    "\n",
    "    retrieved_contextdata = []\n",
    "    for item, context, rating in zip(retrieved_context_ids, retrieved_context, retrieved_context_ratings):\n",
    "        product_context = f\"Product ID: {item} - Description: {context} - Rating: {rating}\"\n",
    "        retrieved_contextdata.append(product_context)\n",
    "    \n",
    "    return json.dumps(retrieved_contextdata), retrieved_contextdata    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36197db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"aggregation_node\", \n",
    "description=\"This function aggregates the retrieved context data and returns the final answer\",\n",
    "run_type=\"retriever\"\n",
    ")\n",
    "def aggregation_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    This function aggregates the retrieved context data and returns the final answer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the formatted product descriptions from state.messages\n",
    "    product_descriptions = []\n",
    "    \n",
    "    for msg in state.messages:\n",
    "      if isinstance(msg, ToolMessage):\n",
    "        if msg.artifact:\n",
    "            for item in msg.artifact:\n",
    "              product_descriptions.append(item)\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a specialized Product Expert Assistant. Your goal is to answer customer questions accurately using ONLY the provided product information.\n",
    "\n",
    "      ### Instructions:\n",
    "      1. **Source of Truth:** Answer strictly based on the provided \"Available Products\" section below. Do not use outside knowledge or make assumptions.\n",
    "      2. **Handling Missing Info:** If the answer cannot be found in the provided products, politely state that you do not have that information. Do not make up features.\n",
    "      3. **Tone:** Be helpful, professional, and concise.\n",
    "      4. **Terminology:** Never refer to the text below as \"context\" or \"data.\" Refer to it naturally as \"our current inventory\" or \"available products.\"\n",
    "      5. **output format** - An output of the following format is expected:\n",
    "          \n",
    "          1. **Answer:** The answer to the question.\n",
    "          2. **Context:** The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "          3. **Description:** Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "      ### Available Products:\n",
    "      <inventory_data>\n",
    "      {{ preprocessed_context }}\n",
    "      </inventory_data>\n",
    "\n",
    "      ### Customer Question:\n",
    "      {{ question }}\n",
    "      \n",
    "      ### Expanded Queries by you on the above question:\n",
    "      {{ expanded_queries }}\n",
    "\n",
    "      ### Answer:\n",
    "      \"\"\"\n",
    "      \n",
    "    prompt = Template(prompt_template).render(\n",
    "                    question=state.user_query, \n",
    "                    expanded_queries=state.expanded_queries, \n",
    "                    preprocessed_context=state.retrieved_contextdata)\n",
    "    \n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    \n",
    "    response, raw_response = client.chat.completions.create_with_completion(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    response_model=AggregationResponse,\n",
    "    temperature=0.4\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      \"answer\": response.answer,\n",
    "      \"retrieved_contextdata\": product_descriptions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add router node to evaluate the user query and decide the next node to execute\n",
    "def router_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    This function evaluates the user query and decides the next node to execute\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    You are a Query Relevance Validator for a specific e-commerce product catalog.\n",
    "    Your job is to classify the user's intent and determine if we need to ask for clarification before proceeding.\n",
    "\n",
    "    ### Instructions:\n",
    "    1. **Analyze Intent:** Look at the User Query below.\n",
    "    2. **Determine Relevance:** - **RELEVANT (true):** The user is asking about buying products, features, comparisons, prices, or inventory.\n",
    "    - **NOT RELEVANT (false):** The user is asking about topics completely unrelated to shopping (e.g., \"How's the weather?\").\n",
    "    3. **Stock & Availability Check:** - If the user asks specifically about **product stock** or **availability** (e.g., \"Is this in stock?\", \"Do you have inventory?\"), you MUST request clarification.\n",
    "    - Set `clarification_needed` to `true`.\n",
    "    - In the `reason` field, draft a polite clarification question (e.g., \"Could you specify which product or store location you are asking about?\").\n",
    "    4. **Output Format:** You must output valid JSON only.\n",
    "\n",
    "    ### Schema:\n",
    "    {\n",
    "        \"query_relevant\": boolean,\n",
    "        \"clarification_needed\": boolean,\n",
    "        \"reason\": \"string\"\n",
    "    }\n",
    "\n",
    "    ### User Query:\n",
    "    {{ question }}\n",
    "\n",
    "    ### JSON Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = Template(prompt_template).render(question=state.user_query)\n",
    "    \n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    \n",
    "    response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=QueryRelevanceResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.4\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query_relevant\": response.query_relevant,\n",
    "        \"answer\": response.reason\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the conditional edge to decide the query expansion or end state\n",
    "from typing import Literal\n",
    "def router_conditional_edge(state: State) -> Literal[\"query_rewriter\", END]:\n",
    "    \"\"\"\n",
    "    This function decides the next node to execute based on the user query\n",
    "    \"\"\"\n",
    "    if state.query_relevant:\n",
    "        return \"query_rewriter\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define agent node which can use RAG pipeline to perform search on the products \n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@traceable(name=\"agent_node\", \n",
    "description=\"This function uses the RAG pipeline to perform search on the products\",\n",
    "run_type=\"retriever\"\n",
    ")\n",
    "def agent_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    This function uses the RAG pipeline to perform search on the products\n",
    "    \"\"\"\n",
    "    search_agent_prompt = \"\"\"\n",
    "    You are a query dispatcher. Your ONLY job is to call the `retrieve_embedding` tool for every input query in the list below.\n",
    "\n",
    "\n",
    "    ### STRICT RULES:\n",
    "    1. **DO NOT** output text, markdown, or python code.\n",
    "    2. **DO NOT** number the list or add bullet points.\n",
    "    3. **MUST** generate a separate Tool Call for each line item.\n",
    "\n",
    "    ### Input Queries:\n",
    "    {{ expanded_queries }}\n",
    "    \"\"\" \n",
    "    \n",
    "    prompt = Template(search_agent_prompt).render(expanded_queries=state.expanded_queries)\n",
    "    \n",
    "    #client = instructor.from_openai(OpenAI())\n",
    "    client = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4).bind_tools([retrieve_embedding], tool_choice=\"required\")\n",
    "    \n",
    "    response = client.invoke(prompt)\n",
    "    #response, raw_response = client.chat.completions.create_with_completion(\n",
    "    #    model=\"gpt-4o-mini\",\n",
    "    #    messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    #    temperature=0.4,\n",
    "    #    tools=[retrieve_embedding]\n",
    "    #)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "# define custom route edge to decide the tool call or agent node or aggregation node\n",
    "def custome_route_edge(state: State) -> Literal[\"ageaggregation_nodent\", \"tools\", END]:\n",
    "    \"\"\"\n",
    "    This function decides the next node to execute based on the user query\n",
    "    \"\"\"\n",
    "    #print(state.messages)\n",
    "    \n",
    "    tool_calls_count = 0\n",
    "    for m in state.messages:\n",
    "        if m.tool_calls and isinstance(m, AIMessage):\n",
    "            tool_calls_count += len(m.tool_calls)\n",
    "    \n",
    "    print(f\"tool_calls_count: {tool_calls_count}\")\n",
    "    \n",
    "    if tool_calls_count == 0:\n",
    "        return \"aggregation\"\n",
    "    \n",
    "    if tools_condition(state.messages) == \"tools\" and tool_calls_count <= len(state.expanded_queries):\n",
    "        return \"tools\"\n",
    "    \n",
    "    return \"aggregation\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af342361",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphbuilder2 = StateGraph(State)\n",
    "\n",
    "tools_node = ToolNode(tools=[retrieve_embedding])\n",
    "graphbuilder2.add_node(\"router\", router_node)\n",
    "graphbuilder2.add_node(\"query_rewriter\", query_rewriter_node)\n",
    "graphbuilder2.add_node(\"agent_node\", agent_node)\n",
    "graphbuilder2.add_node(\"aggregation\", aggregation_node)\n",
    "graphbuilder2.add_node(\"tools\", tools_node)\n",
    "\n",
    "graphbuilder2.add_edge(START, \"router\")\n",
    "graphbuilder2.add_conditional_edges(\"router\", router_conditional_edge, {\"query_rewriter\": \"query_rewriter\", END: END})\n",
    "graphbuilder2.add_edge(\"query_rewriter\", \"agent_node\")\n",
    "graphbuilder2.add_conditional_edges(\"agent_node\", custome_route_edge, {\"tools\": \"tools\", \"aggregation\": \"aggregation\", END: \"aggregation\"})\n",
    "graphbuilder2.add_edge(\"tools\", \"aggregation\")\n",
    "graphbuilder2.add_edge(\"aggregation\", END)\n",
    "\n",
    "agg_graph_1 = graphbuilder2.compile()\n",
    "\n",
    "display(Image(agg_graph_1.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = State(user_query=\"top laptop under 2000$. Show me the kid toys as well. Nice bags for my daughter\")\n",
    "\n",
    "result = agg_graph_1.invoke(initial_state)\n",
    "\n",
    "pprint(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e03231",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
